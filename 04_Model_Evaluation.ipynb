{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClassifyAnything Part 4: Model Evaluation\n",
    "\n",
    "_Supervised learning for classification_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             precision_score, \n",
    "                             recall_score, \n",
    "                             f1_score, \n",
    "                             confusion_matrix,\n",
    "                             roc_curve,\n",
    "                             auc)\n",
    "import joblib\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('always')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n",
    "\n",
    "with open(\"outputs/03_Variables.pkl\", 'rb') as file:\n",
    "    (X_train, X_test, y_train, y_test, kfold) = pickle.load(file)\n",
    "\n",
    "model_logistic_regression = joblib.load('outputs/03-1_Logistic_Regression_final_model.joblib')\n",
    "model_decision_tree = joblib.load('outputs/03-2_Decision_Tree_final_model.joblib')\n",
    "model_random_forest = joblib.load('outputs/03-3_Random_Forest_final_model.joblib')\n",
    "model_SVM = joblib.load('outputs/03-4_SVM_final_model.joblib')\n",
    "\n",
    "trained_models = {\"Logistic Regression\": model_logistic_regression,\n",
    "                  \"Decision Tree\": model_decision_tree,\n",
    "                  \"Random Forest\": model_random_forest,\n",
    "                  \"SVM\": model_SVM}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Benchmark the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to store evaluation metrics for each model\n",
    "model_names = []\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "conf_matrices = []\n",
    "\n",
    "# Loop through each trained model\n",
    "for model_name in trained_models:\n",
    "    # Make predictions on the test dataset\n",
    "    y_pred = trained_models[model_name].predict(X_test)\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Append metrics to lists\n",
    "    model_names.append(model_name)\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "\n",
    "# Create a DataFrame to store the evaluation results\n",
    "evaluation_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Accuracy': accuracies,\n",
    "    'Precision': precisions,\n",
    "    'Recall': recalls,\n",
    "    'F1 Score': f1_scores,\n",
    "    'Confusion Matrix': conf_matrices\n",
    "})\n",
    "\n",
    "# Display the evaluation results\n",
    "print(evaluation_df)\n",
    "\n",
    "# Create the evaluation metrics bar chart using Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(x=model_names, y=accuracies, name='Accuracy'))\n",
    "fig.add_trace(go.Bar(x=model_names, y=f1_scores, name='F1 Score'))\n",
    "fig.add_trace(go.Bar(x=model_names, y=precisions, name='Precision'))\n",
    "fig.add_trace(go.Bar(x=model_names, y=recalls, name='Recall'))\n",
    "\n",
    "fig.update_layout(title='Model Evaluation Metrics',\n",
    "                  xaxis_title='Model',\n",
    "                  yaxis_title='Score',\n",
    "                  barmode='group')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subplot grid to plot multiple confusion matrices\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(trained_models),\n",
    "                         figsize=(15, 4),\n",
    "                         dpi=300)\n",
    "\n",
    "# Plot each confusion matrix using a heatmap\n",
    "for i, model_name in enumerate(trained_models):\n",
    "    sns.heatmap(conf_matrices[i], annot=True, cmap='Blues', fmt='g', ax=axes[i])\n",
    "    axes[i].set_title(model_name)\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "    axes[i].set_ylabel('True')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Create the ROC curve plot using Plotly\n",
    "# For single class\n",
    "fig = go.Figure()\n",
    "\n",
    "for model_name in trained_models:\n",
    "    trained_models[model_name].fit(X_train, y_train)\n",
    "    y_pred_prob = trained_models[model_name].predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Add ROC curve trace for each classifier\n",
    "    fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name=f'{model_name} (AUC = {roc_auc:.2f})'))\n",
    "\n",
    "# Add diagonal line representing random guessing\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', line=dict(color='gray', dash='dash'), name='Random Guessing'))\n",
    "\n",
    "fig.update_layout(title='ROC Curve Comparison for Different Classifiers',\n",
    "                  xaxis_title='False Positive Rate',\n",
    "                  yaxis_title='True Positive Rate',\n",
    "                  legend_title='Classifiers',\n",
    "                  showlegend=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ROC curve plot using Plotly\n",
    "# For multiple classes\n",
    "for model_name in trained_models:\n",
    "    fig = go.Figure()\n",
    "    trained_models[model_name].fit(X_train, y_train)\n",
    "    # Get the probabilities for each class\n",
    "    y_prob = trained_models[model_name].predict_proba(X_test)\n",
    "    # Calculate ROC curve and AUC for each class\n",
    "    for i in range(len(trained_models[model_name].classes_)):\n",
    "        fpr, tpr, _ = roc_curve(y_test == trained_models[model_name].classes_[i], y_prob[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Add ROC curve trace for each class\n",
    "        fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', \n",
    "                                 name=f'{model_name} (Class {trained_models[model_name].classes_[i]}) (AUC = {roc_auc:.2f})'))\n",
    "\n",
    "    # Add diagonal line representing random guessing\n",
    "    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', line=dict(color='gray', dash='dash'), name='Random Guessing'))\n",
    "\n",
    "    fig.update_layout(title='ROC for '+model_name+' (Multi-Class)',\n",
    "                    xaxis_title='False Positive Rate',\n",
    "                    yaxis_title='True Positive Rate',\n",
    "                    xaxis_range=[0,1],yaxis_range=[0,1],\n",
    "                    legend_title='Classifiers',\n",
    "                    showlegend=True,\n",
    "                    plot_bgcolor='white', paper_bgcolor='white',\n",
    "                    xaxis=dict(linecolor='black'),\n",
    "                    yaxis=dict(linecolor='black'),\n",
    "                    legend=dict(orientation=\"h\",\n",
    "                                yanchor=\"top\",y=-0.2,\n",
    "                                xanchor=\"left\",x=-0.2),\n",
    "                    height=500, width=500)\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
