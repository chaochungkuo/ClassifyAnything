{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"ClassifyAnything Part 3-1: Logistic Regression\"\n",
    "author: \"Joseph Chao-Chung Kuo (ckuo@ukaachen.de)\"\n",
    "date: today\n",
    "output:\n",
    "  general:\n",
    "    output_stdout: true\n",
    "    output_error: false\n",
    "    output: true\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "    html-math-method: katex\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "    number_sections: true\n",
    "    embed-resources: true\n",
    "    page-layout: full\n",
    "execute:\n",
    "    echo: true\n",
    "    warning: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Supervised learning for classification_\n",
    "\n",
    "Logistic Regression is a supervised learning algorithm used for binary classification tasks, where the goal is to predict the probability of an input belonging to one of two classes (e.g., Yes/No, True/False, 0/1). Despite its name, logistic regression is a classification algorithm, not a regression algorithm.\n",
    "\n",
    "The key idea behind logistic regression is to model the relationship between the input features and the binary target variable using a logistic function (sigmoid function). The sigmoid function maps any real-valued number to a value between 0 and 1, which represents the probability of the input belonging to the positive class.\n",
    "\n",
    "Mathematically, the logistic regression model can be represented as follows:\n",
    "\n",
    "$$ P(y=1 \\,|\\, x) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "where:\n",
    "- $ P(y=1 \\,|\\, x) $ is the probability of the input $ x $ belonging to the positive class (class 1).\n",
    "- $ z $ is the linear combination of the input features $ x $ and their corresponding weights $ w $ plus a bias term $ b $: $ z = w_1x_1 + w_2x_2 + \\ldots + w_mx_m + b $.\n",
    "- $ e $ is the base of the natural logarithm.\n",
    "\n",
    "During the training phase, logistic regression aims to find the optimal values of the weights $ w $ and bias $ b $ that best fit the training data. This is typically done by minimizing a loss function, such as the binary cross-entropy, using optimization techniques like gradient descent.\n",
    "\n",
    "Once the model is trained, it can be used to make predictions on new data by computing the probability using the logistic function and applying a threshold (e.g., 0.5) to classify the input as either the positive class or the negative class.\n",
    "\n",
    "Logistic regression is a straightforward and interpretable algorithm that works well for linearly separable data and can provide probabilistic predictions, making it useful for many binary classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1.1 Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "with open(\"outputs/03_Variables.pkl\", 'rb') as file:\n",
    "    (X_train, X_test, y_train, y_test, kfold) = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1.2 Train the model with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid for logistic regression. For example, you can tune the C parameter, which controls the regularization strength.\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 50, 100]}\n",
    "\n",
    "# Create an instance of logistic regression and the GridSearchCV object:\n",
    "logistic_model = LogisticRegression(max_iter=1000, multi_class='auto')\n",
    "grid_search = GridSearchCV(logistic_model, param_grid, cv=kfold)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "# Retrieve the best hyperparameters and best model:\n",
    "best_c = grid_search.best_params_['C']\n",
    "best_model = grid_search.best_estimator_\n",
    "# Print the best hyperparameter value:\n",
    "print(\"Best C parameter:\", best_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1.3 Visualize fine-tuning of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the results of the grid search\n",
    "results = grid_search.cv_results_\n",
    "mean_test_scores = results['mean_test_score']\n",
    "params = results['params']\n",
    "param_values = [param['C'] for param in params]\n",
    "\n",
    "# Plot the grid search results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(param_values, mean_test_scores, marker='o', linestyle='-')\n",
    "plt.xlabel('C Parameter')\n",
    "plt.ylabel('Mean Test Score (Accuracy)')\n",
    "plt.title('Grid Search Results')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1.4 Visualize the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "top_N = 20\n",
    "coef_df = pd.DataFrame({\"feature name\": best_model.feature_names_in_,\n",
    "                        \"coefficient\": best_model.coef_[0],\n",
    "                        \"abs\": np.abs(best_model.coef_[0])})\n",
    "coef_df = coef_df.sort_values(by=['abs'], ascending=False)\n",
    "coef_df = coef_df.iloc[0:top_N,:]\n",
    "\n",
    "sns.barplot(data=coef_df, x=\"feature name\", y=\"coefficient\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title('Feature Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1.5 Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# Save the final trained model to a file\n",
    "joblib.dump(best_model, 'outputs/03-1_Logistic_Regression_final_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
