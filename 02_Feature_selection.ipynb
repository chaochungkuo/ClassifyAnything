{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClassifyAnything Part 2: Feature Selection\n",
    "\n",
    "_Supervised learning for classification_\n",
    "\n",
    "Feature selection is the process of selecting a subset of relevant features from the original set of features to improve the performance of a supervised learning classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load the previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "with open(\"outputs/01_Variables.pkl\", 'rb') as file:\n",
    "    (data, labels) = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Removing features with low variance\n",
    "\n",
    "VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance doesnâ€™t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: \t(72, 35140)\n",
      "After filtering:\t(72, 25235)\n",
      "Removed features:\t9905\n"
     ]
    }
   ],
   "source": [
    "# Define the threshold of the proportion of zeros\n",
    "percentage_cf = 0.8\n",
    "selector = VarianceThreshold(threshold=(percentage_cf * (1 - percentage_cf)))\n",
    "filtered_data = selector.fit_transform(data.transpose(), labels)\n",
    "cols_idxs = selector.get_support(indices=True)\n",
    "filtered_data = data.transpose().iloc[:,cols_idxs]\n",
    "print(\"Original shape: \\t\"+str(data.transpose().shape))\n",
    "print(\"After filtering:\\t\"+str(filtered_data.shape))\n",
    "print(\"Removed features:\\t\"+str(data.transpose().shape[1]-filtered_data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to accept this feature selection, please replace data variable\n",
    "data = filtered_data.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Univariate feature selection\n",
    "\n",
    "Univariate feature selection is a type of feature selection method used in machine learning and statistics. It aims to select the most relevant features from a dataset based on their individual relationship with the target variable, without considering the interactions or dependencies between features.\n",
    "\n",
    "In univariate feature selection, each feature is evaluated independently and assigned a score or ranking based on its relationship with the target variable. The scores are then used to select the top-k features that exhibit the strongest relationship with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "X_new = SelectKBest(f_classif, k=1000).fit_transform(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Save variables for next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'outputs'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "# Save variables to a file\n",
    "with open('outputs/02_Variables.pkl', 'wb') as file:\n",
    "    pickle.dump((data, labels), file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is the process of selecting a subset of relevant features from the original set of features to improve the performance of a supervised learning classification model. Here are some commonly used methods for feature selection:\n",
    "\n",
    "1. Univariate Feature Selection:\n",
    "   - This method examines the relationship between each feature and the target variable independently.\n",
    "   - Statistical tests such as chi-square test, ANOVA, or correlation coefficients are used to rank the features based on their relevance to the target.\n",
    "   - SelectKBest and SelectPercentile are popular univariate feature selection techniques.\n",
    "\n",
    "2. Recursive Feature Elimination (RFE):\n",
    "   - RFE is an iterative method that starts with all features and eliminates the least important features in each iteration.\n",
    "   - It uses a model (e.g., logistic regression, support vector machines) to determine the importance of features and eliminates the least important ones.\n",
    "   - RFE continues the elimination process until a desired number of features is reached.\n",
    "   - The sklearn library in Python provides the RFE implementation.\n",
    "\n",
    "3. Feature Importance:\n",
    "   - Some models provide feature importance scores that indicate the relevance or contribution of each feature to the prediction.\n",
    "   - Random Forests and Gradient Boosting models, such as XGBoost and LightGBM, provide feature importance scores.\n",
    "   - Features with higher importance scores are considered more relevant and can be selected for the final model.\n",
    "\n",
    "4. L1 Regularization (Lasso):\n",
    "   - L1 regularization adds a penalty term based on the absolute values of the feature coefficients in the model.\n",
    "   - It encourages sparsity by shrinking less important features' coefficients to zero, effectively performing feature selection.\n",
    "   - Models such as Logistic Regression with L1 regularization can be used for feature selection.\n",
    "\n",
    "5. Principal Component Analysis (PCA):\n",
    "   - PCA is a dimensionality reduction technique that transforms the original features into a new set of uncorrelated variables called principal components.\n",
    "   - By selecting a subset of the principal components that explain most of the variance in the data, feature selection is implicitly performed.\n",
    "   - PCA can be useful when dealing with high-dimensional data or when there is multicollinearity among features.\n",
    "\n",
    "6. Forward/Backward Stepwise Selection:\n",
    "   - Stepwise selection methods build a model iteratively by adding or removing features based on their impact on the model's performance.\n",
    "   - Forward selection starts with an empty set of features and adds one feature at a time, selecting the one that improves the model the most.\n",
    "   - Backward elimination starts with all features and iteratively removes the least significant feature until no further improvement is observed.\n",
    "\n",
    "These are just some of the commonly used methods for feature selection in supervised learning classification tasks. The choice of method depends on the specific problem, dataset characteristics, and the type of model being used. It's often beneficial to experiment with different feature selection techniques and evaluate their impact on the model's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
