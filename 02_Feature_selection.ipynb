{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"ClassifyAnything Part 2: Feature Selection\"\n",
    "author: \"Joseph Chao-Chung Kuo (ckuo@ukaachen.de)\"\n",
    "date: today\n",
    "output:\n",
    "  general:\n",
    "    output_stdout: true\n",
    "    output_error: false\n",
    "    output: true\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 3\n",
    "    html-math-method: katex\n",
    "    code-fold: true\n",
    "    code-tools: true\n",
    "    number_sections: true\n",
    "    embed-resources: true\n",
    "    page-layout: full\n",
    "execute:\n",
    "    echo: true\n",
    "    warning: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Supervised learning for classification_\n",
    "\n",
    "Feature selection is the process of selecting a subset of relevant features from the original set of features to improve the performance of a supervised learning classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load the previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "# Load data and labels\n",
    "with open(\"outputs/01_Variables.pkl\", 'rb') as file:\n",
    "    data, labels = pickle.load(file)\n",
    "data = data.transpose()\n",
    "y = labels[\"Group\"]\n",
    "# Preview data\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_3D(input_data, title, labels):\n",
    "    data_viz = input_data.transpose()\n",
    "    data_long = pd.melt(data_viz, id_vars=data_viz.columns[0], var_name='sample', value_name='mVal')\n",
    "    data_long = pd.merge(data_long, labels, left_on='sample', right_on=\"sample\")\n",
    "    pca = PCA(n_components=3)\n",
    "    pca_data = pca.fit_transform(data_viz.transpose())\n",
    "    pca_data = pd.DataFrame(pca_data)\n",
    "    pca_data.columns = [\"PC 1\", \"PC 2\", \"PC 3\"]\n",
    "    pca_data[\"sample\"] = data_viz.columns\n",
    "    pca_data = pd.merge(pca_data, labels, left_on='sample', right_on=\"label\")\n",
    "    fig = px.scatter_3d(pca_data, x='PC 1', y='PC 2', z='PC 3', color='Group',\n",
    "                        hover_name=\"label\", title=title)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Removing features with low variance\n",
    "\n",
    "VarianceThreshold is a simple baseline approach to feature selection. It removes all features whose variance doesnâ€™t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the threshold of the proportion of zeros\n",
    "percentage_cf = 0.8\n",
    "selector = VarianceThreshold(threshold=(percentage_cf * (1 - percentage_cf)))\n",
    "filtered_data = selector.fit_transform(data, y)\n",
    "cols_idxs = selector.get_support(indices=True)\n",
    "filtered_data = data.iloc[:,cols_idxs]\n",
    "print(\"Original shape: \\t\"+str(data.transpose().shape))\n",
    "print(\"After filtering:\\t\"+str(filtered_data.shape))\n",
    "print(\"Removed features:\\t\"+str(data.transpose().shape[1]-filtered_data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_3D(input_data=filtered_data,  labels=labels,\n",
    "       title=\"Removing features with low variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to accept this feature selection, please replace data variable\n",
    "data = filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Univariate feature selection\n",
    "\n",
    "Univariate feature selection is a type of feature selection method used in machine learning and statistics. It aims to select the most relevant features from a dataset based on their individual relationship with the target variable, without considering the interactions or dependencies between features.\n",
    "\n",
    "In univariate feature selection, each feature is evaluated independently and assigned a score or ranking based on its relationship with the target variable. The scores are then used to select the top-k features that exhibit the strongest relationship with the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 2000  # Number of top features to select\n",
    "k = int(data.shape[1] * 0.1)  # Number of top features to select\n",
    "univariate_selector = SelectKBest(score_func=f_classif, k=k)\n",
    "filtered_data_array = univariate_selector.fit_transform(data, y)\n",
    "# Get the selected feature indices\n",
    "selected_features_indices = univariate_selector.get_support(indices=True)\n",
    "# Convert the filtered data back to a DataFrame\n",
    "filtered_data = pd.DataFrame(filtered_data_array, index=data.index, columns=data.columns[selected_features_indices])\n",
    "print(f\"Shape after univariate feature selection: {filtered_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_3D(input_data=filtered_data,  labels=labels,\n",
    "       title=\"Univariate feature selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to accept this feature selection, please replace data variable\n",
    "data = filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Feature Selection using Mutual Information\n",
    "\n",
    "Mutual Information measures the dependency between variables. It can capture non-linear relationships between features and the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 1000  # Number of top features to select\n",
    "k = int(data.shape[1] * 0.5)  # Number of top features to select\n",
    "mi_selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "filtered_data_array = mi_selector.fit_transform(data, y)\n",
    "# Get the selected feature indices\n",
    "selected_features_indices = mi_selector.get_support(indices=True)\n",
    "# Convert the filtered data back to a DataFrame\n",
    "filtered_data = pd.DataFrame(filtered_data_array, index=data.index, columns=data.columns[selected_features_indices])\n",
    "print(f\"Shape after mutual information selection: {filtered_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_3D(input_data=filtered_data,  labels=labels,\n",
    "       title=\"Mutual information selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to accept this feature selection, please replace data variable\n",
    "data = filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Recursive Feature Elimination (RFE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rfe_selector = RFE(estimator=model, n_features_to_select=k, step=50)\n",
    "filtered_data_array = rfe_selector.fit_transform(data_scaled, labels)\n",
    "# Get the selected feature indices\n",
    "selected_features_indices = rfe_selector.get_support(indices=True)\n",
    "# Convert the filtered data back to a DataFrame\n",
    "filtered_data = pd.DataFrame(filtered_data_array, index=data.index, columns=data.columns[selected_features_indices])\n",
    "print(f\"Shape after RFE selection: {filtered_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_3D(input_data=filtered_data,  labels=labels,\n",
    "       title=\"RFE selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update data if you want to use this feature selection\n",
    "data = filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import colormaps\n",
    "from matplotlib.colors import to_hex\n",
    "\n",
    "colormap = colormaps.get_cmap('tab20')  # Get the colormap without specifying the number of colors\n",
    "colors = [to_hex(colormap(i)) for i in np.linspace(0, 1, len(labels[\"Group\"].unique()))]\n",
    "lut = dict(zip(labels[\"Group\"].unique(), colors))\n",
    "col_colors = labels[\"Group\"].map(lut)\n",
    "col_colors.index = labels[\"label\"]\n",
    "plt.figure(figsize=(10, 8), dpi=600)\n",
    "g = sns.clustermap(data.transpose(), annot=False, cmap='coolwarm', col_colors=col_colors,\n",
    "                   xticklabels=True, yticklabels=True)\n",
    "# Adjust the font size of the x and y tick labels\n",
    "g.ax_heatmap.set_xticklabels(g.ax_heatmap.get_xticklabels(), fontsize=8)  # For x labels\n",
    "g.ax_heatmap.set_yticklabels(g.ax_heatmap.get_yticklabels(), fontsize=8)  # For y labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the feature importances from the model after RFE\n",
    "# The model has been trained on all features, so we need to retrain it on the selected features\n",
    "model.fit(filtered_data, labels)\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Sort the features by importance\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "sorted_features = filtered_data.columns[sorted_indices]\n",
    "sorted_importances = feature_importances[sorted_indices]\n",
    "n = 100\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(f\"Top {min(n, k)} Feature Importances after RFE\")\n",
    "plt.barh(sorted_features[:n], sorted_importances[:n], color=\"royalblue\")\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.yticks(fontsize=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Save variables for next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'outputs'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "# Save variables to a file\n",
    "with open('outputs/02_Variables.pkl', 'wb') as file:\n",
    "    pickle.dump((data, labels, y), file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
